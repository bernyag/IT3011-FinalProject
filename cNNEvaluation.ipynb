{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bitdac362f9e64743bba67d79098195eeb6",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\nCreating input data...\nTrain on 27780 samples, validate on 6945 samples\nEpoch 1/15\n27780/27780 [==============================] - 31s 1ms/step - loss: 2.2441 - accuracy: 0.1765 - val_loss: 2.2337 - val_accuracy: 0.1770\nEpoch 2/15\n27780/27780 [==============================] - 30s 1ms/step - loss: 2.2341 - accuracy: 0.1793 - val_loss: 2.2361 - val_accuracy: 0.1785\nEpoch 3/15\n27780/27780 [==============================] - 31s 1ms/step - loss: 1.4711 - accuracy: 0.4961 - val_loss: 1.0266 - val_accuracy: 0.6514\nEpoch 4/15\n27780/27780 [==============================] - 30s 1ms/step - loss: 0.8592 - accuracy: 0.7298 - val_loss: 0.5701 - val_accuracy: 0.8287\nEpoch 5/15\n27780/27780 [==============================] - 31s 1ms/step - loss: 0.5035 - accuracy: 0.8514 - val_loss: 0.3516 - val_accuracy: 0.8942\nEpoch 6/15\n27780/27780 [==============================] - 31s 1ms/step - loss: 0.3638 - accuracy: 0.8927 - val_loss: 0.2572 - val_accuracy: 0.9217\nEpoch 7/15\n27780/27780 [==============================] - 29s 1ms/step - loss: 0.2939 - accuracy: 0.9127 - val_loss: 0.2366 - val_accuracy: 0.9274\nEpoch 8/15\n27780/27780 [==============================] - 29s 1ms/step - loss: 0.2529 - accuracy: 0.9260 - val_loss: 0.1875 - val_accuracy: 0.9433\nEpoch 9/15\n27780/27780 [==============================] - 29s 1ms/step - loss: 0.2247 - accuracy: 0.9319 - val_loss: 0.1846 - val_accuracy: 0.9450\nEpoch 10/15\n27780/27780 [==============================] - 30s 1ms/step - loss: 0.2029 - accuracy: 0.9401 - val_loss: 0.1560 - val_accuracy: 0.9533\nEpoch 11/15\n27780/27780 [==============================] - 30s 1ms/step - loss: 0.1883 - accuracy: 0.9433 - val_loss: 0.1661 - val_accuracy: 0.9521\nEpoch 12/15\n27780/27780 [==============================] - 30s 1ms/step - loss: 0.1776 - accuracy: 0.9473 - val_loss: 0.1452 - val_accuracy: 0.9545\nEpoch 13/15\n27780/27780 [==============================] - 30s 1ms/step - loss: 0.1626 - accuracy: 0.9516 - val_loss: 0.1455 - val_accuracy: 0.9597\nEpoch 14/15\n27780/27780 [==============================] - 30s 1ms/step - loss: 0.1518 - accuracy: 0.9538 - val_loss: 0.1274 - val_accuracy: 0.9630\nEpoch 15/15\n27780/27780 [==============================] - 30s 1ms/step - loss: 0.1486 - accuracy: 0.9552 - val_loss: 0.1261 - val_accuracy: 0.9624\nCNN Error: 3.76%\n"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "values=['div','times','+','-','0','1','2','3','4','5','6','7','8','9']\n",
    "train_folder = './images_no_copies/'\n",
    "\n",
    "train_data = []\n",
    "labels = []\n",
    "\n",
    "print(\"Creating input data...\")\n",
    "for foldername in os.listdir(train_folder):\n",
    "    for filename in os.listdir(train_folder + foldername):\n",
    "        img = cv2.imread(train_folder + foldername + \"/\" + filename, cv2.IMREAD_GRAYSCALE)\n",
    "        currLabel=values.index(foldername)\n",
    "        resized_img = cv2.resize(img, (45,45))\n",
    "        img_data = resized_img.flatten() / 255 # flatten to 784 and normalize values\n",
    "        train_data.append(img_data)\n",
    "        labels.append(currLabel)\n",
    "\n",
    "train_data = np.asarray(train_data)\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_data, labels, test_size = 0.2, random_state = 101)\n",
    "x_train = x_train.reshape(x_train.shape[0], 45, 45,1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 45, 45,1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "initialsplit=x_train\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(45, 45, 1), activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=15, batch_size=40,shuffle=True)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Confusion Matrix\n"
    }
   ],
   "source": [
    "y_pred=[]\n",
    "for each in x_test:\n",
    "    each=each.reshape(1,45,45,1)\n",
    "    Y_pred = model.predict(each)\n",
    "    y_pred.append(np.argmax(Y_pred, axis=1))\n",
    "\n",
    "rounded_labels=np.argmax(y_test,axis=1)\n",
    "f=open(\"confusionMatrix.txt\",\"w\")\n",
    "print('Confusion Matrix')\n",
    "confusionMat = confusion_matrix(rounded_labels,y_pred,normalize='true')\n",
    "confusionMat=confusionMat*100\n",
    "confusionMat=confusionMat.astype(int)\n",
    "f.write(str(confusionMat))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.9267393772073726\n0.9258261739524242\n0.9250635153292681\n0.9624190064794816\n"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "print(precision_score(rounded_labels, y_pred , average=\"macro\"))\n",
    "print(recall_score(rounded_labels, y_pred , average=\"macro\"))\n",
    "print(f1_score(rounded_labels, y_pred , average=\"macro\"))\n",
    "print(accuracy_score(rounded_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('model.h5')\n",
    "#print(\"Model saved as model.h5\")\n",
    "filename = 'model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  }
 ]
}