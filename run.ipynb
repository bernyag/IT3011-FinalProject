{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n14\nWARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n\n"
    }
   ],
   "source": [
    "# Simple CNN for the MNIST Dataset\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "values=['div','times','+','-','0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "# load dataset\n",
    "file = np.genfromtxt('./equations/good.csv', delimiter=',')\n",
    "# file = csv.reader(\"good.csv\", delimiter=',')\n",
    "data = file[1:, 1:]\n",
    "changedData=[]\n",
    "for img in data:\n",
    "\timg=img.reshape(45,46)\n",
    "\timg=np.resize(img,(45,45))\n",
    "\timg=img.flatten()/255\n",
    "\tchangedData.append(img)\n",
    "changedData=np.asarray(changedData)\n",
    "\n",
    "labellist = []\n",
    "with open('./equations/good.csv') as f:\n",
    "    reader = csv.reader(f, delimiter=\",\")\n",
    "    for i in reader:\n",
    "        labellist.append(list(i)[0])\n",
    "labels = np.asarray(labellist)[1:]\n",
    "newLabels=[]\n",
    "for each in labels:\n",
    "\tcurrLabel=values.index(each)\n",
    "\tnewLabels.append(currLabel)\n",
    "labels=np.asarray(newLabels)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(changedData, labels, test_size = 0.2, random_state = 101)\n",
    "x_train = x_train.reshape(x_train.shape[0], 45, 45,1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 45, 45,1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "print(num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(45, 45, 1), activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nTrain on 1120 samples, validate on 280 samples\nEpoch 1/15\n1120/1120 [==============================] - 3s 3ms/step - loss: 2.6408 - accuracy: 0.0536 - val_loss: 2.6410 - val_accuracy: 0.0500\nEpoch 2/15\n1120/1120 [==============================] - 2s 2ms/step - loss: 2.6390 - accuracy: 0.0848 - val_loss: 2.6420 - val_accuracy: 0.0500\nEpoch 3/15\n1120/1120 [==============================] - 2s 2ms/step - loss: 2.6388 - accuracy: 0.0768 - val_loss: 2.6429 - val_accuracy: 0.0500\nEpoch 4/15\n1120/1120 [==============================] - 2s 2ms/step - loss: 2.6388 - accuracy: 0.0768 - val_loss: 2.6453 - val_accuracy: 0.0500\nEpoch 5/15\n1120/1120 [==============================] - 2s 2ms/step - loss: 2.6388 - accuracy: 0.0750 - val_loss: 2.6464 - val_accuracy: 0.0500\nEpoch 6/15\n1120/1120 [==============================] - 2s 2ms/step - loss: 2.6387 - accuracy: 0.0750 - val_loss: 2.6466 - val_accuracy: 0.0500\nEpoch 7/15\n1120/1120 [==============================] - 2s 2ms/step - loss: 2.6385 - accuracy: 0.0795 - val_loss: 2.6460 - val_accuracy: 0.0500\nEpoch 8/15\n1120/1120 [==============================] - 2s 2ms/step - loss: 2.6383 - accuracy: 0.0688 - val_loss: 2.6473 - val_accuracy: 0.0500\nEpoch 9/15\n1120/1120 [==============================] - 2s 2ms/step - loss: 2.6385 - accuracy: 0.0759 - val_loss: 2.6478 - val_accuracy: 0.0500\nEpoch 10/15\n1120/1120 [==============================] - 2s 2ms/step - loss: 2.6388 - accuracy: 0.0652 - val_loss: 2.6468 - val_accuracy: 0.0500\nEpoch 11/15\n1120/1120 [==============================] - 2s 2ms/step - loss: 2.6384 - accuracy: 0.0777 - val_loss: 2.6477 - val_accuracy: 0.0500\nEpoch 12/15\n1120/1120 [==============================] - 2s 2ms/step - loss: 2.6385 - accuracy: 0.0732 - val_loss: 2.6485 - val_accuracy: 0.0500\nEpoch 13/15\n1120/1120 [==============================] - 2s 2ms/step - loss: 2.6386 - accuracy: 0.0777 - val_loss: 2.6486 - val_accuracy: 0.0500\nEpoch 14/15\n1120/1120 [==============================] - 2s 2ms/step - loss: 2.6382 - accuracy: 0.0732 - val_loss: 2.6485 - val_accuracy: 0.0500\nEpoch 15/15\n1120/1120 [==============================] - 2s 2ms/step - loss: 2.6387 - accuracy: 0.0732 - val_loss: 2.6481 - val_accuracy: 0.0500\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x169fd85fa20>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=15, batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(x_test)\n",
    "prediction_classes = [None]*len(predictions)\n",
    "\n",
    "for i, p in enumerate(predictions):\n",
    "    prediction_classes[i] = values[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CNN Error: 95.00%\n"
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(scores)\n",
    "# print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}